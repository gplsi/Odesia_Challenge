# Odesia Challenge - GPLSI Experiments

![License](https://img.shields. io/badge/License-Apache%202.0-blue.svg)
![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.5.1+-red.svg)
![CUDA](https://img.shields. io/badge/CUDA-12.1+-green.svg)

## üö© Purpose of This Repository

This repository is dedicated to the **Odesia Challenge**, an initiative that benchmarks language technologies across 10 distinct NLP tasks in Spanish. Developed and maintained by the **GPLSI group** (Grupo de Procesamiento del Lenguaje y Sistemas de la Informaci√≥n), the goal is to build and evaluate a **unified system capable of handling all these tasks efficiently**, using advanced machine learning strategies that do **not require separate architectures for each task**.

Rather than approaching each task as pure generation, the challenge allows solutions via **token classification**, enabling decoder-only, encoder-only, or encoder-decoder models. The GPLSI experiments documented and implemented here specifically focus on **decoder-only language models**, optimising their performance across tasks using:

- **Fine-tuning** on specific tasks
- **Zero-shot learning** without task-specific training
- **Retrieval-Augmented Generation (RAG)** with hybrid search capabilities

### The Core Question

> *"How far can decoder-only language models go when solving varied NLP tasks in Spanish, using strategic adaptation and retrieval techniques?"*

This repository documents the GPLSI group's comprehensive answer to this question through rigorous experimentation, systematic comparison, and production-ready implementation.

---

## üìã Table of Contents

- [What is the Odesia Challenge?](#what-is-the-odesia-challenge)
- [Why This Repository Exists](#why-this-repository-exists)
- [Our Experimental Approaches](#our-experimental-approaches)
- [Repository Structure](#repository-structure)
- [System Architecture](#system-architecture)
- [Installation & Setup](#installation--setup)
- [Experimentation Workflows](#experimentation-workflows)
- [Results & Performance](#results--performance)
- [Technologies & Stack](#technologies--stack)
- [Docker Deployment](#docker-deployment)
- [Contributing](#contributing)
- [License](#license)
- [References](#references)

---

## üèÜ What is the Odesia Challenge?

The **Odesia Challenge** is a comprehensive Spanish NLP benchmark initiative designed to evaluate system performance across **10 diverse linguistic tasks** within a single, unified framework.

### Challenge Characteristics

- **10 Spanish NLP Tasks:** Diverse linguistic phenomena covering multiple NLP subtasks
- **Single System Requirement:** Build ONE unified system instead of separate models per task
- **Flexible Solution Approaches:** Tasks can be solved through generation, token classification, or sequence labeling, it can include complex systems with RAG and agents as well.
- **Practical Focus:** Emphasis on real-world applicability and computational efficiency
- **Multi-model Support:** Encoder, decoder, and encoder-decoder architectures welcome

### Why It Matters

The Odesia Challenge addresses a critical gap in NLP research:

> **"Can modern language models obtain a good performance across different competitive tasks in Spanish?"**

Spanish is a language with rich morphosyntactic complexity, regional variations, and specific linguistic phenomena.  The challenge bridges the gap between academic language understanding and practical, deployable NLP systems.

---

## üìñ Why This Repository Exists

### Primary Objectives

1. **Benchmark Development:** Create reference implementations for handling multiple Spanish NLP tasks
2. **Strategy Comparison:** Systematically compare fine-tuning, zero-shot, and RAG-based approaches
3. **Model Optimization:** Optimize decoder-only models for multi-task Spanish NLP
4. **Reproducibility:** Provide fully documented, reproducible experiments with version-controlled configurations
5. **Production Deployment:** Enable easy deployment and serving of models via containerized infrastructure
6. **Knowledge Sharing:** Document findings and best practices for the community

### What You'll Find Here

‚úÖ **Complete training pipelines** for task-specific fine-tuning  
‚úÖ **Zero-shot evaluation framework** to measure transfer learning capabilities  
‚úÖ **Advanced RAG system** with hybrid search (vector + semantic) using Weaviate  
‚úÖ **Experiment tracking** with systematic result documentation  
‚úÖ **Production infrastructure** with Docker, FastAPI, and Gradio support  
‚úÖ **Comprehensive evaluation metrics** and comparative analysis tools  
‚úÖ **Reusable utility modules** for data processing and model management  

---

## üî¨ Our Experimental Approaches

The GPLSI group implements **three distinct strategies**, each testing different hypotheses about decoder-only model capabilities:

### 1Ô∏è‚É£ **Fine-Tuning on Specific Tasks**

**Hypothesis:** Adapting decoder models to task-specific Spanish data improves performance.

**Characteristics:**
- **Mechanism:** Task-specific training with token classification heads
- **Location:** `experiments/fine_tuning/`
- **Use Case:** Baseline performance and upper bound estimation
- **Training:** Gradient-based optimization on task-specific data

**Strengths:**
- Optimal individual task performance
- Leverages task-specific patterns and nuances
- Provides upper bound for comparison

**Limitations:**
- Requires training per task (computational cost)
- Higher data requirements
- Risk of overfitting to specific tasks

**When to Use:**
- Maximum performance is critical
- Sufficient task-specific training data available
- Computational resources permit per-task training

---

### 2Ô∏è‚É£ **Zero-Shot Experiments**

**Hypothesis:** Pretrained decoder models possess sufficient Spanish linguistic knowledge for direct task solving.

**Characteristics:**
- **Mechanism:** Direct inference on tasks without any task-specific training
- **Location:** `experiments/zero_shot/`
- **Use Case:** Measuring inherent model capabilities and transfer learning
- **Inference:** Single forward pass, no adaptation

**Strengths:**
- No task-specific training required (fastest path to results)
- Demonstrates transfer learning capability
- Minimal computational overhead
- Tests true generalization ability

**Limitations:**
- Lower individual task performance
- Depends heavily on pretraining quality
- Limited by model's inherent understanding
- No task-specific optimization

**When to Use:**
- Quick baseline results needed
- Limited training resources
- Evaluating model generalization
- Assessing multilingual capabilities

---

### 3Ô∏è‚É£ **Retrieval-Augmented Generation (RAG) with Few-Shot Learning**

**Hypothesis:** Augmenting inference with dynamically retrieved relevant examples dramatically improves performance across diverse tasks.

**Characteristics:**
- **Mechanism:** Hybrid search (vector + semantic) with Weaviate for dynamic example retrieval
- **Location:** `experiments/` (various numbered experiments) and `src/retrieval/`
- **Use Case:** Balancing performance and adaptability across multiple tasks
- **Inference:** Few-shot prompting with retrieved relevant examples

**Architecture:**
- **Vector Database:** Weaviate for efficient similarity search
- **Embedding Service:** Sentence transformers for encoding examples
- **RAG Pipeline:** Dynamic retrieval and context augmentation
- **Few-Shot Learning:** In-context learning with retrieved examples

**Strengths:**
- Improved performance without task-specific training
- Flexible adaptation across tasks
- Leverages existing labeled data efficiently
- Better generalization through diverse examples

**Limitations:**
- Requires vector database infrastructure
- Retrieval quality affects performance
- Increased inference latency
- Dependency on example database quality

**When to Use:**
- Need flexibility across multiple tasks
- Limited training resources but have task examples
- Quick adaptation to new tasks required
- Balance between performance and efficiency

---

## üìÇ Repository Structure

```
Odesia_Challenge/
‚îú‚îÄ‚îÄ üìÑ README.md              # This comprehensive documentation
‚îú‚îÄ‚îÄ üìÑ LICENSE                # Apache 2.0 License
‚îú‚îÄ‚îÄ üìÑ requirements.txt       # Python dependencies
‚îú‚îÄ‚îÄ üìÑ Dockerfile             # Container definition for deployment
‚îú‚îÄ‚îÄ üìÑ docker-compose.yml     # Multi-container orchestration
‚îú‚îÄ‚îÄ üìÑ Makefile               # Build automation and common tasks
‚îÇ
‚îú‚îÄ‚îÄ üìÅ src/                   # Source code (main application logic)
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ main.py           # Application entry point
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ utils.py          # Common utility functions
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ inference_api.py  # API inference endpoint (stub)
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ langchain_script.py # LangChain integration
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ data/             # Data configuration and task definitions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ base.py       # Base task classes and interfaces
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ config.py     # Task configurations and dataset definitions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ prompt_syntax.py # Prompt formatting utilities
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ README_EN.md  # Data documentation (English)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ tasks/        # Individual task implementations
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ base.py   # Task base classes
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ diann_2023_t1.py      # DIANN 2023 Task 1
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ dipromats_t1.py       # DIPROMATS Task 1
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ dipromats_t2.py       # DIPROMATS Task 2
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ dipromats_t3.py       # DIPROMATS Task 3
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ exist_2022_t1.py      # EXIST 2022 Task 1
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ exist_2022_t2.py      # EXIST 2022 Task 2
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ exist_2023_t1.py      # EXIST 2023 Task 1
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ exist_2023_t2.py      # EXIST 2023 Task 2
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ exist_2023_t3.py      # EXIST 2023 Task 3
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ üìÑ sqac_squad_2024_t1.py # SQAC SQuAD 2024 Task 1
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ preprocessing/    # Data preprocessing modules
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ diann_2023_t1.py  # DIANN preprocessing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ exist_2023_t1.py  # EXIST T1 preprocessing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ exist_2023_t2.py  # EXIST T2 preprocessing
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ exist_2023_t3.py  # EXIST T3 preprocessing
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ retrieval/        # RAG and vector database components
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py          # Retrieval module initialization
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ embedding_service.py # Embedding generation service
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ rag_service.py       # RAG orchestration logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ schema.py            # Weaviate schema definitions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ setup_database.py    # Database initialization
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ _tests.py            # Retrieval system tests
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ evaluation/       # Evaluation and metrics
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ (evaluation modules)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ postprocessing/   # Output postprocessing
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ (postprocessing modules)
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ scripts/          # Automation and utility scripts
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ encode_dataset.py     # Dataset encoding for retrieval
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ eval.py               # Evaluation runner
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ generate_csvs.py      # CSV generation utilities
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ init_retrieval.py     # Retrieval system initialization
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ langchain_pipeline.py # LangChain pipeline implementation
‚îÇ       ‚îî‚îÄ‚îÄ üìÑ test_model.py         # Model testing utilities
‚îÇ
‚îú‚îÄ‚îÄ üìÅ experiments/          # Experiment configurations and runners
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ 01_finetuned_llama3b_instruct_0shot_bio.sh
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ 02_finetuned_llama3b_instruct_0shot_nonbio.sh
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ 03_finetuned_llama8b_instructed_0shot_nonbio.sh
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ 04_finetuned_llama3b_base_0shot_nonbio.sh
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ 05_finetuned_llama3b_base_5shot_nonbio.sh
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ 06_pretrained_llama8b_base_5shot_non_bio.sh
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ 07_finetuned_salamanda8b_instructed_0shot_nonbio.sh
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ 08_finetuned_deepseekr1_instructed_0shot_nonbio.sh
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ 09_pretrained_salamandra8b_base_5shot_nonbio.sh
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ 10_pretrained_deepseekr1_base_5shot_nonbio.sh
‚îÇ
‚îú‚îÄ‚îÄ üìÅ scripts/              # Repository-level automation scripts
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ init_all.sh      # Initialize all tasks and datasets
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ proccess_all.sh  # Process all datasets
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ encode_all.sh    # Encode all datasets for retrieval
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ generate_csvs.sh # Generate evaluation CSVs
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ post_val.sh      # Post-validation processing
‚îÇ
‚îú‚îÄ‚îÄ üìÅ utils/                # Utility scripts and helpers
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ creating_gold.py # Gold standard generation
‚îÇ
‚îî‚îÄ‚îÄ üìÅ data/                 # Data directory (gitignored)
    ‚îî‚îÄ‚îÄ (task datasets and outputs)
```

### Key Components Explained

#### **`src/data/`** - Task Configuration & Definitions
Central hub for task definitions, dataset configurations, and prompt formatting. Each task in `src/data/tasks/` implements the necessary interfaces for data loading, preprocessing, and evaluation.

**Supported Tasks:**
- **DIANN 2023 T1:** Diagnostic annotation task
- **DIPROMATS T1-T3:** Diplomatic text analysis tasks
- **EXIST 2022/2023 T1-T3:** Sexism identification and classification
- **SQAC SQuAD 2024 T1:** Spanish Question Answering

#### **`src/preprocessing/`** - Data Preprocessing
Task-specific preprocessing modules that handle data cleaning, tokenization, and format standardization. Uses `rich` for progress tracking and consistent console output.

#### **`src/retrieval/`** - RAG Infrastructure
Complete RAG implementation with:
- **Vector Database:** Weaviate integration for similarity search
- **Embedding Service:** Sentence transformer models for encoding
- **RAG Service:** Orchestration logic for retrieval-augmented generation
- **Schema Management:** Weaviate collection schemas

#### **`src/scripts/`** - Automation Tools
Reusable scripts for common workflows:
- Dataset encoding for vector search
- Model evaluation orchestration
- CSV result generation
- Retrieval system initialization
- LangChain pipeline integration

#### **`experiments/`** - Experiment Runners
Numbered shell scripts representing different experimental configurations:
- **Zero-shot (0shot):** Direct inference without examples
- **Few-shot (5shot):** In-context learning with retrieved examples
- **Fine-tuned:** Models adapted to specific tasks
- **Pretrained:** Base models without task-specific training
- **Model variants:** LLaMA (3B/8B), Salamandra (8B), DeepSeek R1

#### **`scripts/`** - Repository Automation
High-level scripts for managing the entire pipeline:
- **`init_all.sh`**: Initialize all datasets and prepare environment
- **`proccess_all.sh`**: Run all preprocessing pipelines
- **`encode_all.sh`**: Encode datasets for retrieval
- **`generate_csvs.sh`**: Generate evaluation results
- **`post_val.sh`**: Post-validation analysis

---

## üèóÔ∏è System Architecture

### High-Level Architecture

```mermaid
graph TB
    subgraph Input["üì• Input Layer"]
        Raw[Raw Spanish Text]
        Task[Task Definition]
    end
    
    subgraph Processing["‚öôÔ∏è Processing Layer"]
        Preprocess[Preprocessing Module]
        TaskConfig[Task Configuration]
    end
    
    subgraph Inference["ü§ñ Inference Layer"]
        subgraph Strategies["Inference Strategies"]
            FT[Fine-Tuning]
            ZS[Zero-Shot]
            RAG[RAG + Few-Shot]
        end
        
        Model[Decoder-Only LLM]
        VectorDB[(Weaviate Vector DB)]
    end
    
    subgraph Output["üì§ Output Layer"]
        Postprocess[Postprocessing]
        Eval[Evaluation Metrics]
        Results[Results & CSVs]
    end
    
    Raw --> Preprocess
    Task --> TaskConfig
    Preprocess --> FT
    Preprocess --> ZS
    Preprocess --> RAG
    TaskConfig --> FT
    TaskConfig --> ZS
    TaskConfig --> RAG
    
    FT --> Model
    ZS --> Model
    RAG --> VectorDB
    VectorDB --> Model
    
    Model --> Postprocess
    Postprocess --> Eval
    Eval --> Results
```

### Component Interactions

1. **Data Flow:** Raw data ‚Üí Preprocessing ‚Üí Task-specific formatting ‚Üí Model inference ‚Üí Postprocessing ‚Üí Evaluation
2. **RAG Flow:** Query ‚Üí Embedding ‚Üí Vector search ‚Üí Example retrieval ‚Üí Context augmentation ‚Üí Generation
3. **Experiment Flow:** Configuration ‚Üí Dataset preparation ‚Üí Model loading ‚Üí Inference ‚Üí Result collection ‚Üí Analysis

---

## üöÄ Installation & Setup

### Prerequisites

- **Python:** 3.8 or higher
- **CUDA:** 12.1+ (for GPU acceleration)
- **PyTorch:** 2.5.1+ with CUDA support
- **Docker:** (optional) For containerized deployment
- **Weaviate:** (for RAG experiments) Vector database instance

### Installation Steps

#### 1. Clone the Repository

```bash
git clone https://github.com/gplsi/Odesia_Challenge.git
cd Odesia_Challenge
```

#### 2. Set Up Python Environment

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

#### 3. Initialize Data and Environment

```bash
# Initialize all datasets and prepare environment
bash scripts/init_all.sh

# Process all datasets
bash scripts/proccess_all.sh

# (Optional) Encode datasets for RAG
bash scripts/encode_all.sh
```

#### 4. Set Up Vector Database (for RAG experiments)

```bash
# Start Weaviate using Docker
docker-compose up -d

# Initialize retrieval system
python src/scripts/init_retrieval.py
```

### Docker Deployment

```bash
# Build Docker image
docker build -t odesia-challenge .

# Run with docker-compose
docker-compose up

# Or run standalone
docker run -p 8000:8000 odesia-challenge
```

---

## üß™ Experimentation Workflows

### Running Fine-Tuning Experiments

```bash
# Example: Fine-tuned LLaMA 3B with zero-shot inference
bash experiments/01_finetuned_llama3b_instruct_0shot_bio.sh
```

### Running Zero-Shot Evaluation

```bash
# Example: Pretrained LLaMA 8B with zero-shot
bash experiments/03_finetuned_llama8b_instructed_0shot_nonbio.sh
```

### Running RAG Experiments

```bash
# Example: Few-shot with RAG retrieval
bash experiments/05_finetuned_llama3b_base_5shot_nonbio.sh
```

### Custom Evaluation

```python
from src.scripts.eval import run_evaluation

# Run evaluation on specific task
results = run_evaluation(
    task_name="exist_2023_t1",
    model_name="llama3b",
    strategy="rag",
    num_shots=5
)
```

### Generating Results

```bash
# Generate CSV results for all experiments
bash scripts/generate_csvs.sh

# Post-validation analysis
bash scripts/post_val.sh
```

---

## üìä Results & Performance

> **Note:** Detailed results and performance metrics will be added as experiments are completed.

### Evaluation Metrics

- **Task-specific metrics:** Precision, Recall, F1-Score
- **Overall performance:** Macro/Micro averaging across tasks
- **Efficiency metrics:** Inference time, memory usage
- **Comparative analysis:** Strategy comparison (Fine-tuning vs. Zero-shot vs. RAG)

---

## üõ†Ô∏è Technologies & Stack

### Core Technologies

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Language Models** | LLaMA 3B/8B, Salamandra 8B, DeepSeek R1 | Decoder-only models for text generation |
| **Deep Learning** | PyTorch 2.5.1+ | Neural network framework |
| **Vector Database** | Weaviate | Similarity search and retrieval |
| **Embeddings** | Sentence Transformers | Text encoding for retrieval |
| **API Framework** | FastAPI | RESTful API endpoints |
| **UI Framework** | Gradio | Interactive web interface |
| **Containerization** | Docker | Deployment and orchestration |
| **Progress Display** | Rich | Beautiful console output and progress bars |

### Python Libraries

```
torch>=2.5.1
transformers
datasets
weaviate-client
sentence-transformers
langchain
fastapi
gradio
rich
numpy
pandas
scikit-learn
```

---

## üê≥ Docker Deployment

### Building and Running

```bash
# Build the image
make build

# Run the application
make run

# Run with docker-compose (includes Weaviate)
make up

# Stop services
make down
```

### Environment Variables

Configure the following in your `.env` file or docker-compose:

```bash
WEAVIATE_URL=http://localhost:8080
MODEL_PATH=/models/llama-3b
CUDA_VISIBLE_DEVICES=0
BATCH_SIZE=8
```

---

## ü§ù Contributing

Contributions are welcome! Please follow these guidelines:

1. **Fork the repository** and create a feature branch
2. **Follow code standards:** Use type hints, docstrings, and consistent formatting
3. **Add tests:** Ensure new features have corresponding tests
4. **Document changes:** Update README and relevant documentation
5. **Submit a Pull Request:** With a clear description of changes

### Development Setup

```bash
# Install development dependencies
pip install -r requirements-dev.txt

# Run tests
pytest tests/

# Format code
black src/
isort src/
```

---

## üìú License

This project is licensed under the **Apache License 2.0**. See the [LICENSE](LICENSE) file for details.

---

## üìö References

### Odesia Challenge
- [Challenge Documentation](#) *([link](https://leaderboard.odesia.uned.es/leaderboard/challenge))*

### Models Used
- **LLaMA:** [Meta AI LLaMA](https://ai.meta.com/llama/)
- **Salamandra:** [BSC Salamandra](https://huggingface.co/BSC-LT)
- **DeepSeek R1:** [DeepSeek AI](https://www.deepseek.com/)

### Technologies
- **Weaviate:** [weaviate.io](https://weaviate.io/)
- **LangChain:** [python.langchain.com](https://python.langchain.com/)
- **PyTorch:** [pytorch.org](https://pytorch.org/)

### GPLSI Group
- **Homepage:** [Grupo de Procesamiento del Lenguaje y Sistemas de la Informaci√≥n](https://gplsi.dlsi.ua.es/)
- **University of Alicante:** [ua.es](https://www.ua.es/)

---

## üë• Authors & Acknowledgments

**Developed by:** GPLSI Group (Grupo de Procesamiento del Lenguaje y Sistemas de la Informaci√≥n)  
**Institution:** University of Alicante, Spain

Special thanks to all contributors and the organizers of the Odesia Challenge for creating this comprehensive Spanish NLP benchmark.

---

## üìû Contact & Support

For questions, issues, or collaboration inquiries:

- **GitHub Issues:** [Open an issue](https://github.com/gplsi/Odesia_Challenge/issues)
- **Email:** *(contact email to be added)*
- **GPLSI Website:** [gplsi.dlsi.ua.es](https://gplsi.dlsi.ua.es/)

---

**Last Updated:** December 2025